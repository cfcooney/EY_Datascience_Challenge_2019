{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score, f1_score \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions required for classification.\n",
    "\"\"\"\n",
    "def accuracy_f_score(y_pred,y_true):\n",
    "    \"\"\"\n",
    "    Prints both accuracy and f-score, given predictions and true values.\n",
    "    \"\"\"\n",
    "    print(f\"Accuracy score: {round(accuracy_score(y_true, y_pred) * 100,2)}%\")\n",
    "    print('\\033[92m' + f\"F1 score: {f1_score(y_true, y_pred)}\" + '\\033[0m')\n",
    "    \n",
    "def in_city(x_pred,y_pred):\n",
    "    \"\"\"\n",
    "    Computes whether given coordinates are within the city-centre or not.\n",
    "    \"\"\"\n",
    "    if (3750901.5068 <= x_pred <= 3770901.5069) and (-19268905.6133 <= y_pred <= -19208905.6133):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(1)\n",
    "    y = 1/(1+e**(-x))\n",
    "    return y\n",
    "\n",
    "def journey_time(x,y):\n",
    "    \"\"\"\n",
    "    Compute journey time in seconds.\n",
    "    \"\"\"\n",
    "    x = pd.to_datetime(x)\n",
    "    y = pd.to_datetime(y)\n",
    "    return (y-x).total_seconds()\n",
    "\n",
    "def to_binary(x):\n",
    "    result = []\n",
    "    for n in x:\n",
    "        result.append(np.argmax(n))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_entry</th>\n",
       "      <th>time_exit</th>\n",
       "      <th>vmax</th>\n",
       "      <th>vmin</th>\n",
       "      <th>vmean</th>\n",
       "      <th>x_entry</th>\n",
       "      <th>y_entry</th>\n",
       "      <th>x_exit</th>\n",
       "      <th>y_exit</th>\n",
       "      <th>dist</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_tr</th>\n",
       "      <th>x_home</th>\n",
       "      <th>y_home</th>\n",
       "      <th>nj</th>\n",
       "      <th>dist_pct_ch</th>\n",
       "      <th>j_time</th>\n",
       "      <th>dpc</th>\n",
       "      <th>time_x</th>\n",
       "      <th>time_y</th>\n",
       "      <th>home</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajectory_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>traj_0000a8602cf2def930488dee7cdad104_1_5</th>\n",
       "      <td>15:02:31</td>\n",
       "      <td>15:18:33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.744945e+06</td>\n",
       "      <td>-1.928183e+07</td>\n",
       "      <td>3.744785e+06</td>\n",
       "      <td>-1.928148e+07</td>\n",
       "      <td>45797.982227</td>\n",
       "      <td>...</td>\n",
       "      <td>3544.948847</td>\n",
       "      <td>3.751014e+06</td>\n",
       "      <td>-1.909398e+07</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.071843</td>\n",
       "      <td>962.0</td>\n",
       "      <td>0.482047</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>-1113.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0000cf177130469eeac79f67b6bcf3df_9_3</th>\n",
       "      <td>15:00:32</td>\n",
       "      <td>15:29:48</td>\n",
       "      <td>1.149404</td>\n",
       "      <td>1.149404</td>\n",
       "      <td>1.149404</td>\n",
       "      <td>3.749088e+06</td>\n",
       "      <td>-1.926605e+07</td>\n",
       "      <td>3.749610e+06</td>\n",
       "      <td>-1.926594e+07</td>\n",
       "      <td>29603.985176</td>\n",
       "      <td>...</td>\n",
       "      <td>270.043451</td>\n",
       "      <td>3.749450e+06</td>\n",
       "      <td>-1.926506e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.009039</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>0.497740</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-1788.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0001f97b99a80f18f62e2d44e54ef33d_3_1</th>\n",
       "      <td>14:34:35</td>\n",
       "      <td>15:19:51</td>\n",
       "      <td>30.167742</td>\n",
       "      <td>30.167742</td>\n",
       "      <td>30.167742</td>\n",
       "      <td>3.758738e+06</td>\n",
       "      <td>-1.937594e+07</td>\n",
       "      <td>3.769687e+06</td>\n",
       "      <td>-1.914258e+07</td>\n",
       "      <td>137051.659155</td>\n",
       "      <td>...</td>\n",
       "      <td>-1867.319643</td>\n",
       "      <td>3.771461e+06</td>\n",
       "      <td>-1.910413e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013813</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>0.503453</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>-1191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0002124248b0ca510dea42824723ccac_31_10</th>\n",
       "      <td>15:28:54</td>\n",
       "      <td>15:28:54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.767866e+06</td>\n",
       "      <td>-1.917797e+07</td>\n",
       "      <td>3.767866e+06</td>\n",
       "      <td>-1.917797e+07</td>\n",
       "      <td>61336.955341</td>\n",
       "      <td>...</td>\n",
       "      <td>-59655.060438</td>\n",
       "      <td>3.765544e+06</td>\n",
       "      <td>-1.917227e+07</td>\n",
       "      <td>9.0</td>\n",
       "      <td>35.468958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1734.0</td>\n",
       "      <td>-1734.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_000219c2a6380c307e8bffd85b5e404b_23_16</th>\n",
       "      <td>15:08:05</td>\n",
       "      <td>15:08:05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.747641e+06</td>\n",
       "      <td>-1.922695e+07</td>\n",
       "      <td>3.747641e+06</td>\n",
       "      <td>-1.922695e+07</td>\n",
       "      <td>17851.785279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.760336e+06</td>\n",
       "      <td>-1.922818e+07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-485.0</td>\n",
       "      <td>-485.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            time_entry time_exit       vmax  \\\n",
       "trajectory_id                                                                 \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     15:02:31  15:18:33   0.000000   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3     15:00:32  15:29:48   1.149404   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1     14:34:35  15:19:51  30.167742   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10   15:28:54  15:28:54   0.000000   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16   15:08:05  15:08:05   0.000000   \n",
       "\n",
       "                                                  vmin      vmean  \\\n",
       "trajectory_id                                                       \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     0.000000   0.000000   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3     1.149404   1.149404   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    30.167742  30.167742   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10   0.000000   0.000000   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16   0.000000   0.000000   \n",
       "\n",
       "                                                  x_entry       y_entry  \\\n",
       "trajectory_id                                                             \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    3.744945e+06 -1.928183e+07   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    3.749088e+06 -1.926605e+07   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    3.758738e+06 -1.937594e+07   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  3.767866e+06 -1.917797e+07   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  3.747641e+06 -1.922695e+07   \n",
       "\n",
       "                                                   x_exit        y_exit  \\\n",
       "trajectory_id                                                             \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    3.744785e+06 -1.928148e+07   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    3.749610e+06 -1.926594e+07   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    3.769687e+06 -1.914258e+07   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  3.767866e+06 -1.917797e+07   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  3.747641e+06 -1.922695e+07   \n",
       "\n",
       "                                                      dist  ...   \\\n",
       "trajectory_id                                               ...    \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     45797.982227  ...    \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3     29603.985176  ...    \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    137051.659155  ...    \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10   61336.955341  ...    \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16   17851.785279  ...    \n",
       "\n",
       "                                                  prev_tr        x_home  \\\n",
       "trajectory_id                                                             \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     3544.948847  3.751014e+06   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3      270.043451  3.749450e+06   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    -1867.319643  3.771461e+06   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10 -59655.060438  3.765544e+06   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16      0.000000  3.760336e+06   \n",
       "\n",
       "                                                   y_home   nj  dist_pct_ch  \\\n",
       "trajectory_id                                                                 \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5   -1.909398e+07  6.0    -0.071843   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3   -1.926506e+07  4.0    -0.009039   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1   -1.910413e+07  2.0     0.013813   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10 -1.917227e+07  9.0    35.468958   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16 -1.922818e+07  8.0     0.000000   \n",
       "\n",
       "                                             j_time       dpc  time_x  time_y  \\\n",
       "trajectory_id                                                                   \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     962.0  0.482047  -151.0 -1113.0   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    1756.0  0.497740   -32.0 -1788.0   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    2716.0  0.503453  1525.0 -1191.0   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10     0.0  1.000000 -1734.0 -1734.0   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16     0.0  0.500000  -485.0  -485.0   \n",
       "\n",
       "                                             home  \n",
       "trajectory_id                                      \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5       0  \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3       0  \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1       0  \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10     0  \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16     1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data_train/all_features.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dpc</th>\n",
       "      <th>home</th>\n",
       "      <th>start_CC</th>\n",
       "      <th>net_tr_b</th>\n",
       "      <th>prev_tr_b</th>\n",
       "      <th>odd_even_nj</th>\n",
       "      <th>dist_scaled</th>\n",
       "      <th>jt_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.482047</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238078</td>\n",
       "      <td>0.050261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153702</td>\n",
       "      <td>0.091745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713536</td>\n",
       "      <td>0.141902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.319040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dpc  home  start_CC  net_tr_b  prev_tr_b  odd_even_nj  dist_scaled  \\\n",
       "0  0.482047     0         0         1          1            1     0.238078   \n",
       "1  0.497740     0         0         0          1            1     0.153702   \n",
       "2  0.503453     0         0         0          0            1     0.713536   \n",
       "3  1.000000     0         0         1          0            0     0.319040   \n",
       "4  0.500000     1         0         0          0            1     0.092470   \n",
       "\n",
       "   jt_scaled  \n",
       "0   0.050261  \n",
       "1   0.091745  \n",
       "2   0.141902  \n",
       "3   0.000000  \n",
       "4   0.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute y using 'e_exit' and 'y_exit'.\n",
    "\"\"\"\n",
    "df[\"final_loc\"] = list(map(in_city, df[\"x_exit\"], df[\"y_exit\"]))\n",
    "y = df[\"final_loc\"].values\n",
    "\n",
    "df = pd.read_csv('../data_train/binary_features.csv') #binary features computed previously.\n",
    "\n",
    "#df = df[(np.abs(stats.zscore(df)) < 5).all(axis=1)] #removes outlier if required.\n",
    "df.drop([\"final_loc\",\"j_time\",\"dist_pct_ch\",\"dist\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature transformations -- Not required here when using the binary featureset.\n",
    "\"\"\"\n",
    "df = df.apply(abs,axis=1)\n",
    "df = df.apply(np.log10,axis=1)\n",
    "df[df == -np.inf] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X = df.values\n",
    "# X = scaler.fit_transform(X) #similarly not required when using binary featureset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we can implement SMOTE or ADASYN for class-balancing. Both improved \n",
    "performance significantly within the training set tests. However, on the \n",
    "out-of-sample test-set performance was degraded. These methods are not \n",
    "implemented here.\n",
    "\"\"\"\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "s = SMOTE(sampling_strategy='minority', random_state=10, k_neighbors=3)\n",
    "#s = ADASYN()\n",
    "#df.fillna(0, inplace=True)\n",
    "X, y = s.fit_resample(df, y)\n",
    "\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement learning-rate scheduler for learning-rate decay.\n",
    "\"\"\"\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = .005\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "activation_1 (LeakyReLU)     (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "activation_2 (LeakyReLU)     (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_3 (LeakyReLU)     (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "BN3 (BatchNormalization)     (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_4 (LeakyReLU)     (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 2,380,034\n",
      "Trainable params: 2,373,378\n",
      "Non-trainable params: 6,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep neural network model used for classification, consisting of an input layer,\n",
    "an output layer and 3 fully-connected hidden layers. LeakyReLU used as \n",
    "activation function and dropout not used as it did not improve performance.\n",
    "Instead 'class_weight' is used to provide information on imbalanced classes to \n",
    "the classifier, below.\n",
    "\"\"\"\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',input_dim = X.shape[1], activation=None, name='layer1'))\n",
    "NN_model.add(keras.layers.LeakyReLU(alpha=0.3, name='activation_1'))\n",
    "NN_model.add(BatchNormalization(name='BN1'))\n",
    "#NN_model.add(Dropout(0.1, name='DO1'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(1024, kernel_initializer='normal',activation=None, kernel_regularizer=keras.regularizers.l1(0.000),\n",
    "                   name='fc1'))\n",
    "NN_model.add(keras.layers.LeakyReLU(alpha=0.3, name='activation_2'))\n",
    "NN_model.add(BatchNormalization(name='BN2'))\n",
    "#NN_model.add(Dropout(0.1, name='DO2'))\n",
    "\n",
    "\n",
    "NN_model.add(Dense(1024, kernel_initializer='normal',activation=None, kernel_regularizer=keras.regularizers.l1(0.000),\n",
    "                   name='fc2'))\n",
    "NN_model.add(keras.layers.LeakyReLU(alpha=0.3, name='activation_3'))\n",
    "NN_model.add(BatchNormalization(name='BN3'))\n",
    "#NN_model.add(Dropout(0.1, name='DO3'))\n",
    "\n",
    "NN_model.add(Dense(1024, kernel_initializer='normal',activation=None, kernel_regularizer=keras.regularizers.l1(0.000),\n",
    "                   name='fc3'))\n",
    "NN_model.add(keras.layers.LeakyReLU(alpha=0.3, name='activation_4'))\n",
    "NN_model.add(BatchNormalization(name='BN4'))\n",
    "#NN_model.add(Dropout(0.1, name='DO3'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(2, activation='softmax', name='classifier'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=.0, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "NN_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 85783 samples, validate on 21446 samples\n",
      "Epoch 1/50\n",
      "85783/85783 [==============================] - 39s 457us/step - loss: 0.2320 - acc: 0.8557 - val_loss: 0.2717 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27170, saving model to Weights-001--0.27170.hdf5\n",
      "Epoch 2/50\n",
      "85783/85783 [==============================] - 38s 444us/step - loss: 0.1891 - acc: 0.8664 - val_loss: 0.3848 - val_acc: 0.7183\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "85783/85783 [==============================] - 37s 435us/step - loss: 0.1839 - acc: 0.8687 - val_loss: 0.1770 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.27170 to 0.17695, saving model to Weights-003--0.17695.hdf5\n",
      "Epoch 4/50\n",
      "85783/85783 [==============================] - 38s 437us/step - loss: 0.1720 - acc: 0.8717 - val_loss: 0.1743 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17695 to 0.17429, saving model to Weights-004--0.17429.hdf5\n",
      "Epoch 5/50\n",
      "85783/85783 [==============================] - 37s 430us/step - loss: 0.1606 - acc: 0.8774 - val_loss: 0.1555 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.17429 to 0.15547, saving model to Weights-005--0.15547.hdf5\n",
      "Epoch 6/50\n",
      "85783/85783 [==============================] - 37s 427us/step - loss: 0.1587 - acc: 0.8774 - val_loss: 0.1549 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15547 to 0.15494, saving model to Weights-006--0.15494.hdf5\n",
      "Epoch 7/50\n",
      "85783/85783 [==============================] - 37s 430us/step - loss: 0.1580 - acc: 0.8784 - val_loss: 0.1610 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "85783/85783 [==============================] - 42s 488us/step - loss: 0.1558 - acc: 0.8786 - val_loss: 0.1655 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "85783/85783 [==============================] - 42s 493us/step - loss: 0.1555 - acc: 0.8800 - val_loss: 0.1621 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "85783/85783 [==============================] - 38s 445us/step - loss: 0.1501 - acc: 0.8843 - val_loss: 0.1433 - val_acc: 0.8872\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15494 to 0.14327, saving model to Weights-010--0.14327.hdf5\n",
      "Epoch 11/50\n",
      "85783/85783 [==============================] - 39s 454us/step - loss: 0.1498 - acc: 0.8826 - val_loss: 0.1756 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "85783/85783 [==============================] - 38s 449us/step - loss: 0.1490 - acc: 0.8829 - val_loss: 0.1723 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "85783/85783 [==============================] - 37s 431us/step - loss: 0.1505 - acc: 0.8831 - val_loss: 0.1492 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "85783/85783 [==============================] - 38s 439us/step - loss: 0.1483 - acc: 0.8843 - val_loss: 0.1506 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "85783/85783 [==============================] - 37s 436us/step - loss: 0.1459 - acc: 0.8858 - val_loss: 0.1405 - val_acc: 0.8866\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.14327 to 0.14050, saving model to Weights-015--0.14050.hdf5\n",
      "Epoch 16/50\n",
      "85783/85783 [==============================] - 37s 426us/step - loss: 0.1451 - acc: 0.8867 - val_loss: 0.1447 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "85783/85783 [==============================] - 37s 432us/step - loss: 0.1457 - acc: 0.8859 - val_loss: 0.1494 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "85783/85783 [==============================] - 37s 434us/step - loss: 0.1448 - acc: 0.8865 - val_loss: 0.1490 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "85783/85783 [==============================] - 37s 426us/step - loss: 0.1439 - acc: 0.8868 - val_loss: 0.1579 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "85783/85783 [==============================] - 37s 434us/step - loss: 0.1422 - acc: 0.8879 - val_loss: 0.1378 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.14050 to 0.13779, saving model to Weights-020--0.13779.hdf5\n",
      "Epoch 21/50\n",
      "85783/85783 [==============================] - 37s 437us/step - loss: 0.1417 - acc: 0.8887 - val_loss: 0.1436 - val_acc: 0.8849\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "85783/85783 [==============================] - 37s 436us/step - loss: 0.1415 - acc: 0.8885 - val_loss: 0.1427 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "85783/85783 [==============================] - 36s 420us/step - loss: 0.1413 - acc: 0.8887 - val_loss: 0.1397 - val_acc: 0.8877\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/50\n",
      "85783/85783 [==============================] - 38s 440us/step - loss: 0.1415 - acc: 0.8888 - val_loss: 0.1434 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      "85783/85783 [==============================] - 38s 441us/step - loss: 0.1403 - acc: 0.8891 - val_loss: 0.1380 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/50\n",
      "85783/85783 [==============================] - 37s 427us/step - loss: 0.1400 - acc: 0.8889 - val_loss: 0.1365 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.13779 to 0.13654, saving model to Weights-026--0.13654.hdf5\n",
      "Epoch 27/50\n",
      "85783/85783 [==============================] - 37s 435us/step - loss: 0.1397 - acc: 0.8888 - val_loss: 0.1386 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/50\n",
      "85783/85783 [==============================] - 38s 441us/step - loss: 0.1392 - acc: 0.8888 - val_loss: 0.1376 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/50\n",
      "85783/85783 [==============================] - 41s 481us/step - loss: 0.1394 - acc: 0.8897 - val_loss: 0.1367 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/50\n",
      "85783/85783 [==============================] - 38s 446us/step - loss: 0.1389 - acc: 0.8894 - val_loss: 0.1374 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/50\n",
      "85783/85783 [==============================] - 37s 432us/step - loss: 0.1382 - acc: 0.8898 - val_loss: 0.1363 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.13654 to 0.13632, saving model to Weights-031--0.13632.hdf5\n",
      "Epoch 32/50\n",
      "85783/85783 [==============================] - 36s 418us/step - loss: 0.1384 - acc: 0.8902 - val_loss: 0.1360 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.13632 to 0.13596, saving model to Weights-032--0.13596.hdf5\n",
      "Epoch 33/50\n",
      "85783/85783 [==============================] - 36s 420us/step - loss: 0.1388 - acc: 0.8896 - val_loss: 0.1358 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.13596 to 0.13578, saving model to Weights-033--0.13578.hdf5\n",
      "Epoch 34/50\n",
      "85783/85783 [==============================] - 39s 455us/step - loss: 0.1384 - acc: 0.8907 - val_loss: 0.1389 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/50\n",
      "85783/85783 [==============================] - 46s 536us/step - loss: 0.1378 - acc: 0.8906 - val_loss: 0.1360 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/50\n",
      "85783/85783 [==============================] - 45s 528us/step - loss: 0.1376 - acc: 0.8901 - val_loss: 0.1357 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.13578 to 0.13570, saving model to Weights-036--0.13570.hdf5\n",
      "Epoch 37/50\n",
      "85783/85783 [==============================] - 50s 583us/step - loss: 0.1378 - acc: 0.8898 - val_loss: 0.1357 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.13570 to 0.13568, saving model to Weights-037--0.13568.hdf5\n",
      "Epoch 38/50\n",
      "85783/85783 [==============================] - 47s 544us/step - loss: 0.1379 - acc: 0.8901 - val_loss: 0.1359 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/50\n",
      "85783/85783 [==============================] - 48s 557us/step - loss: 0.1375 - acc: 0.8899 - val_loss: 0.1359 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85783/85783 [==============================] - 42s 487us/step - loss: 0.1373 - acc: 0.8907 - val_loss: 0.1353 - val_acc: 0.8908\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.13568 to 0.13525, saving model to Weights-040--0.13525.hdf5\n",
      "Epoch 41/50\n",
      "85783/85783 [==============================] - 43s 501us/step - loss: 0.1377 - acc: 0.8903 - val_loss: 0.1354 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/50\n",
      "85783/85783 [==============================] - 42s 491us/step - loss: 0.1372 - acc: 0.8910 - val_loss: 0.1352 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.13525 to 0.13521, saving model to Weights-042--0.13521.hdf5\n",
      "Epoch 43/50\n",
      "85783/85783 [==============================] - 44s 512us/step - loss: 0.1373 - acc: 0.8905 - val_loss: 0.1355 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/50\n",
      "85783/85783 [==============================] - 42s 487us/step - loss: 0.1374 - acc: 0.8901 - val_loss: 0.1360 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/50\n",
      "85783/85783 [==============================] - 42s 490us/step - loss: 0.1371 - acc: 0.8910 - val_loss: 0.1355 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/50\n",
      "85783/85783 [==============================] - 41s 481us/step - loss: 0.1370 - acc: 0.8912 - val_loss: 0.1353 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/50\n",
      "85783/85783 [==============================] - 41s 476us/step - loss: 0.1372 - acc: 0.8907 - val_loss: 0.1354 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/50\n",
      "85783/85783 [==============================] - 41s 472us/step - loss: 0.1370 - acc: 0.8905 - val_loss: 0.1357 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/50\n",
      "85783/85783 [==============================] - 40s 464us/step - loss: 0.1372 - acc: 0.8904 - val_loss: 0.1356 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/50\n",
      "85783/85783 [==============================] - 41s 481us/step - loss: 0.1369 - acc: 0.8910 - val_loss: 0.1353 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x294fc6310f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' #name for saving model weights\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint,loss_history,lrate]\n",
    "class_weight = {0: .5,\n",
    "                1: .66} # class-balancing\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=410,shuffle=True)\n",
    "\n",
    "y_train = to_categorical(y_train) #converts y-labels to format required with loss function.\n",
    "\n",
    "NN_model.fit(x_train, y_train, epochs=50, batch_size=256, validation_split = 0.2,\n",
    "             callbacks=callbacks_list, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'Weights-042--0.13521.hdf5' # choose the best checkpoint 'Weights-049--0.09069.hdf5'\n",
    "NN_model.load_weights(weights_file) # load it\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 89.28%\n",
      "\u001b[92mF1 score: 0.8092109629039749\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pred = NN_model.predict(x_test)\n",
    "pred_b = to_binary(pred)\n",
    "y_test = to_categorical(y_test)\n",
    "y_test_b = to_binary(y_test)\n",
    "accuracy_f_score(pred_b,y_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
