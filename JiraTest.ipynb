{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jira import JIRA\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couldn't get this to work - maybe don't have credentials or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn=JIRA('https://jira.atlassian.com')\n",
    "# project_str='project=\"SPARK\"'\n",
    "# issues=conn.search_issues(project_str,startAt=0, maxResults=1)\n",
    "\n",
    "# for issue in issues:\n",
    "#     pprint.pprint(issue.raw[\"fields\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This might help with some of the Graph errors, I was getting them too\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # suppresses tensorflow logs - only use when needed\n",
    "os.environ['HDF5_DISABLE_VERSION_CHECK'] = '2'\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on this tutorial with some changes: https://www.tensorflow.org/text/tutorials/text_classification_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "#                           as_supervised=True)\n",
    "# train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text, label in train_dataset.take(1):\n",
    "#     print(text)\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just pulled the description text from these bugs and suggestions from Jira to test with \n",
    "# 6 bugs, 5 suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = [\"Steps to Reproduce: Add attachment to page and copy URL Log out Enter URL Log In Expected Results: The attachment should download. Actual Results: The attachment doesn't download, and user is given a 'Not Permitted' error. Workaround: Entering the URL again after logging in will allow the download to proceed\",\n",
    "             \"When attempting to embed a intranet web link in a jira issue, using the format : http://server.subdomain1.subdomain2.example.net/mywebapp\",\n",
    "             \"At the moment when a User Macro contain the following:<p><ac:link><ri:page ri:content-title=Welcome to Confluence' /></ac:link></p> The resulting macro will translate the above, to a link as below: When a user follows the link it leads to an empty page. The user can then add content to the page, this page remains accessible by the above link until this page is renamed. When the page is renamed, the above link will lead to an empty page again with the same title. This behaviour means that page links generated by User Macros, are not dynamic with every page rename. The Ask: Allow page links generated through User Macros to be dynamic with every page rename. gives the error: Invalid 'URL'. Make sure you include the full URL (e.g. http://www.google.com).\",\n",
    "             \"Issue Summary On Profile settings page, the 'Show option panel...' checkbox input is not associated with its visible label programmatically in the disabled state. Tested URL: 'https://confsrva11ya.instenv.atl-test.space/users/viewmysettingsaction' Steps to reproduce Navigate to the confluence homepage. Navigate to the profile image button inside header section & activate it. Activate 'Settings' link from the list. Navigate to the\",\n",
    "             \"Issue Summary On Profile settings page, the 'Enable' checkbox input is not associated with its visible label programmatically in the disabled state. Tested URL https://confsrva11ya.instenv.atl-test.space/users/viewmysettings.action Steps to reproduce Navigate to the confluence homepage. Navigate to the profile image button inside header section & activate it. Activate 'Settings' link from the list. Navigate to the 'Enable' checkbox in main content. Actual Results While navigating with screen reader tuned on, when user navigate to the 'Enable' checkbox screen reader announces as 'Dimmed tick, tickbox' because the visual label is not programatically associated with input field. This makes screen reader users difficult to understand the purpose of these input fields. Expected Results The screen reader should announce the labels when user navigate to the 'Enable' checkbox. Associate the labels to the input fields explicitly via for & id relation. Code Snippet:\",\n",
    "             \"Problem XStream is vulnerable to security exploits such as highlighted in the image attached. The list of CVEs can be found in https://x-stream.github.io/security.html This ticket tracks its upgrade to 1.4.18. Environment Confluence v7.13 Workaround Set xstream.allowlist.enable sysprop to true. This is equivalent to XStream 1.4.18 behaviour and it exist in Confluence 7.10 and up. But it comes with a risk of broken third-party plugins which have not yet configured xstream-security module with their classes. Confirm with Third-party plugin vendors before toggling it if your Confluence instance uses a third-party plugin and it relies on XStream.\",\n",
    "             \"When using question list macro with 'Include Ask a Question button' enabled in the confluence page. @ mention feature does not work. Steps to Reproduce Question for Confluence app should be enabled in Confluence. Created a new page.Inserted a 'Question list' Macro and checked 'Include Ask a Question button' insert and publish the page.\",\n",
    "             \"NOTE: This suggestion is for Confluence Server. Using Confluence Cloud? See the corresponding suggestion. Something that would improve usability is to add keep editing mode when adding/updating a page. I.e. [Add and Keep Editing Page] button. When working on the web, I always have the fear of having my browser crashing. Using such a feature (in combination with a publish/draft feature) might be interesting. I could save at anytime, without fearing losing all my work. workaround: If you hit Preview a draft is saved automatically.\",\n",
    "             \"NOTE: This suggestion is for Confluence Server. Using Confluence Cloud? See the corresponding suggestion. We would like an easy way to highlight any text on a page (individual words, characters, sentences, or paragraphs, standalone or within a table or header, etc.) with color through the WYSIWYG editor, applied as a formatting option much like Bold, Italic, or Underline are currently applied. An entire palette of colors isn't necessary, but even 2 or 3 options would be tremendously useful. There are ways to highlight table rows and columns or individual cells, and through plugins there are ways to highlight individual words via the {highlight} macro. But this plugin/macro is not terribly efficient when trying highlight multiple, non-adjacent words. The basis for this functionality may already exist, as email notifications and version comparisons show changes in highlighted text of different colors. Example of desired output is attached in 'ExamplesOfHighlightedText.jpg'.\",\n",
    "             \"NOTE: This suggestion is for Confluence Server. Using Confluence Cloud? See the corresponding suggestion. At the moment our supported platforms doc does not specify if we support clustered versions of supported databases, eg. Oracle RAC, MySQL Cluster,MSSQL Cluster & etc. While they might work with Confluence, we do not specifically test against them. Supporting clustered databases can be an attractive feature for Enterprise customers. Workaround While this functionality is not currently guaranteed nor supported, if MS SQL is clusterd, you can try this format to see if it will work with your environment.\",\n",
    "              \"Unknown Users appears in various places such as Mentions: And everywhere else. This causes degraded functions: Mentioned users are not notified Permissions and restrictions are not properly set - eg, users unable to edit pages How to replicate The issue happens when we rename LDAP users to usernames that already exists in Confluence. For example: Set a user directory connection to LDAP A with username attribute: SAMAccountname Set another user directory connection to LDAP A with username attribute: mail Edit the second user directory's username attribute to SAMAccountname Expected Result Since the newly renamed users are being renamed to existing set of users, it's expected that the users are now 'merged'. Actual Result Duplicated users exists in the user_mapping table. One of them has NULL lower_username and causing sightings of Unknown User in Confluence UI. Notes When connecting to a new user directory with a new set of usernames, Confluence will create records of these users in the user_mapping table. When user are renamed, Confluence will update user records in the user_mapping table. This is where the issue happens as users are being renamed to an already existing username and forced one of the records' lower_username to be NULL to avoid duplicate constraint in the database: user_mapping table after connecting to 2 LDAP with different usern\"]\n",
    "fake_labels = ['Bug', \"Suggestion\", \"Suggestion\", \"Bug\", \"Bug\", \"Bug\", \"Bug\",\"Suggestion\",\"Suggestion\", \"Suggestion\", \"Bug\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the labels as ints. 0,1 here - will be 0-10 for yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(fake_labels)\n",
    "numeric_labels = le.transform(fake_labels)\n",
    "print(numeric_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vectorization - this one is fairly new so if it doesn't work might be a tensorflow version issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'to', 'a', 'in', 'confluence', 'with', 'page',\n",
       "       'is', 'user', 'this', 'and', 'of', 'when', 'not', 'users', 'are',\n",
       "       'navigate', 'it', 'for', 'url', 'link', 'be', 'as', 'will',\n",
       "       'using', 'table', 'suggestion', 'set'], dtype='<U61')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model - I put the last dense layer to 2, you will do 11\n",
    "# This is layer is different to the tutorial becuase it was causing errors\n",
    "## -- tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64)))\n",
    "## I added softmax at the end as it can help with classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(64))),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model takes the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50402147 0.4959785 ]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "## - I changed the loss tgo sparse categorical cross entropy but you can try others\n",
    "## - Adam optimizer is probably the best/most common. \n",
    "## - Varying the learning rate (1e-4) can have significant effect\n",
    "\n",
    "## Accuracy might skew a bit when you have all the classes since they're unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy']) # may want to try different metrics - accuracy can be misleading with imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things like batch size (should be much higher with full dataset) and epochs and other things here can help: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.6945 - accuracy: 0.2727\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6905 - accuracy: 0.7273\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6873 - accuracy: 0.8182\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.6845 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6818 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6792 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6763 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6733 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.6704 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6668 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=np.array(fake_data), y=numeric_labels, batch_size=2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test samples for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bug = [\"his error happens because the full site backup procedure does not have a progress bar to indicate the state of the operation. When the backup is started, the browser keeps waiting for the backup to complete as in the following screenshot (notice Waiting for host message at the lower left corner of the screen): f the instance is fairly large, the backup will take longer than the proxy timeout configured and result in the Proxy Error message shown previously. This is because there is no response from the application while the backup is being created. Notice that the backup operation is not impacted. If you wait a for its completion, the backup will be available either in <confluence-home>/backups or <confluence-home>/temp. However, we do not have any indication of the process state besides checking if the backup file stopped growing and is zipped in the directory above. A possible solution would be having a progress bar to this process, as we have for the XML import procedure. This also affects other types of exports, like PDF ones.\"]\n",
    "\n",
    "test_suggestion = [\"The confluence editor removes legitimate HTML attributes to start the numbered list entries at a particular start number. For example: Demonstrated here: https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_ol_start, Renders a list numbered from 8. This is correct HTML entered in Confluence with the Confluence Source Editor After pressing [*Apply*] in the editor the page editor display and the preview display show the list beginning with one. Whe  This is offered as the only solution for this problem in the Q&A section: How to continue a numbered list back in 2013, Five years ago Removal of this ability from HTML appears to be a REGRESSION error in the editor.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bug']\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([test_bug])\n",
    "np.argmax(pred)\n",
    "print(le.inverse_transform([np.argmax(pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Suggestion']\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([\"When attempting to embed a intranet web link in a jira issue, using the format : http://server.subdomain1.subdomain2.example.net/mywebapp\"])\n",
    "np.argmax(pred)\n",
    "print(le.inverse_transform([np.argmax(pred)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try messing around different model configurations \n",
    "## Here I addes a dense layer, some dropout layers and changed the activation function to leaky_relu (it is more common now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(128))),\n",
    "    tf.keras.layers.Dense(128, activation=None),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(64, activation=None),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.6921 - accuracy: 0.4545\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.6876 - accuracy: 0.6364\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.6876 - accuracy: 0.5455\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6835 - accuracy: 0.6364\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.6808 - accuracy: 0.6364\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6769 - accuracy: 0.8182\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6714 - accuracy: 0.9091\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.6673 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6598 - accuracy: 0.9091\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6550 - accuracy: 0.8182\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=np.array(fake_data), y=numeric_labels, batch_size=2, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
