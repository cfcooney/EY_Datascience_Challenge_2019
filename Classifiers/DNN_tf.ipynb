{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_f_score(y_pred,y_true):\n",
    "    print(f\"Accuracy score: {round(accuracy_score(y_true, y_pred) * 100,2)}%\")\n",
    "    print('\\033[92m' + f\"F1 score: {f1_score(y_true, y_pred)}\" + '\\033[0m')\n",
    "    \n",
    "\n",
    "def in_city(y_pred):\n",
    "    targets = []\n",
    "    for pred in y_pred:\n",
    "        if (3750901.5068 <= pred[0] <= 3770901.5069) and (-19268905.6133 <= pred[1] <= -19208905.6133):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "    return targets\n",
    "\n",
    "def journey_time(x,y):\n",
    "    \"\"\"\n",
    "    Compute journey time in seconds.\n",
    "    \"\"\"\n",
    "    x = pd.to_datetime(x)\n",
    "    y = pd.to_datetime(y)\n",
    "    return (y-x).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'data_train/data_train.csv\\')\\ntrain_df = df[df[\"time_exit\"][:].str[:2] == \\'15\\']\\ntrain_df[\"j_time\"] = list(map(journey_time, train_df[\"time_entry\"], train_df[\"time_exit\"]))\\ntrain_df.drop([\"vmax\",\"vmin\",\"vmean\",\"time_entry\",\"time_exit\",\"hash\",\"Unnamed: 0\"], axis=1, inplace=True)\\ntrain_df.to_csv(\\'data_train/3_features_final.csv\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv('data_train/data_train.csv')\n",
    "train_df = df[df[\"time_exit\"][:].str[:2] == '15']\n",
    "train_df[\"j_time\"] = list(map(journey_time, train_df[\"time_entry\"], train_df[\"time_exit\"]))\n",
    "train_df.drop([\"vmax\",\"vmin\",\"vmean\",\"time_entry\",\"time_exit\",\"hash\",\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "train_df.to_csv('data_train/3_features_final.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data_train/3_features_final.csv')\n",
    "\n",
    "y = [train_df[\"x_exit\"].values, train_df[\"y_exit\"].values]\n",
    "y = np.transpose(np.array(y))\n",
    "\n",
    "train_df.set_index(\"trajectory_id\", inplace=True)\n",
    "train_df.drop([\"x_exit\",\"y_exit\",\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "X = train_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 20\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.reshape((134037,3))\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=410,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, kernel_initializer='normal', activation=None, name='Dense_1',\n",
    "                kernel_regularizer=keras.regularizers.l2(l=0.001)))\n",
    "model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(1024, kernel_initializer='normal', activation=None, name='Dense_2',\n",
    "                kernel_regularizer=keras.regularizers.l2(l=0.001)))\n",
    "model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(1024, kernel_initializer='normal', activation=None, name='Dense_3',\n",
    "                kernel_regularizer=keras.regularizers.l2(l=0.001)))\n",
    "model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "\n",
    "\n",
    "model.add(Dense(2, kernel_initializer='normal', activation='linear', name='Classifier'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = f\"Atlanta-CC-DNN-{int(time.time())}\"\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85783 samples, validate on 21446 samples\n",
      "Epoch 1/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 37.6380 - acc: 0.9952\n",
      "Epoch 00001: val_loss improved from inf to 13.32528, saving model to Weights-001--13.32528.hdf5\n",
      "85783/85783 [==============================] - 18s 213us/sample - loss: 37.5757 - acc: 0.9952 - val_loss: 13.3253 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 13.3317 - acc: 1.0000\n",
      "Epoch 00002: val_loss improved from 13.32528 to 8.98538, saving model to Weights-002--8.98538.hdf5\n",
      "85783/85783 [==============================] - 18s 205us/sample - loss: 13.3368 - acc: 1.0000 - val_loss: 8.9854 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 15.5801 - acc: 1.0000\n",
      "Epoch 00003: val_loss improved from 8.98538 to 6.73349, saving model to Weights-003--6.73349.hdf5\n",
      "85783/85783 [==============================] - 16s 190us/sample - loss: 15.5679 - acc: 1.0000 - val_loss: 6.7335 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 9.3553 - acc: 1.0000\n",
      "Epoch 00004: val_loss improved from 6.73349 to 5.50423, saving model to Weights-004--5.50423.hdf5\n",
      "85783/85783 [==============================] - 16s 191us/sample - loss: 9.3470 - acc: 1.0000 - val_loss: 5.5042 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 9.0687 - acc: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 5.50423\n",
      "85783/85783 [==============================] - 16s 189us/sample - loss: 9.0963 - acc: 1.0000 - val_loss: 19.2279 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 9.1125 - acc: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 5.50423\n",
      "85783/85783 [==============================] - 18s 209us/sample - loss: 9.1162 - acc: 1.0000 - val_loss: 22.4827 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 14.7220 - acc: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 5.50423\n",
      "85783/85783 [==============================] - 15s 181us/sample - loss: 14.7035 - acc: 1.0000 - val_loss: 9.0280 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 5.9356 - acc: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 5.50423\n",
      "85783/85783 [==============================] - 15s 176us/sample - loss: 5.9320 - acc: 1.0000 - val_loss: 6.1162 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 5.5116 - acc: 1.0000\n",
      "Epoch 00009: val_loss improved from 5.50423 to 3.93519, saving model to Weights-009--3.93519.hdf5\n",
      "85783/85783 [==============================] - 15s 178us/sample - loss: 5.5172 - acc: 1.0000 - val_loss: 3.9352 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 13.8966 - acc: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 187us/sample - loss: 13.8841 - acc: 1.0000 - val_loss: 6.8533 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 6.5966 - acc: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 18s 204us/sample - loss: 6.5923 - acc: 1.0000 - val_loss: 6.5287 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 6.5616 - acc: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 190us/sample - loss: 6.5544 - acc: 1.0000 - val_loss: 4.2078 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 4.0972 - acc: 1.0000\n",
      "Epoch 00013: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 188us/sample - loss: 4.0944 - acc: 1.0000 - val_loss: 4.0666 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 7.8049 - acc: 1.0000\n",
      "Epoch 00014: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 188us/sample - loss: 7.8091 - acc: 1.0000 - val_loss: 10.5495 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 6.5622 - acc: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 184us/sample - loss: 6.5669 - acc: 1.0000 - val_loss: 6.3136 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 5.9710 - acc: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 3.93519\n",
      "85783/85783 [==============================] - 16s 188us/sample - loss: 5.9632 - acc: 1.0000 - val_loss: 5.9643 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 4.8452 - acc: 1.0000\n",
      "Epoch 00017: val_loss improved from 3.93519 to 3.31258, saving model to Weights-017--3.31258.hdf5\n",
      "85783/85783 [==============================] - 16s 187us/sample - loss: 4.8402 - acc: 1.0000 - val_loss: 3.3126 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 5.5129 - acc: 1.0000\n",
      "Epoch 00018: val_loss did not improve from 3.31258\n",
      "85783/85783 [==============================] - 16s 189us/sample - loss: 5.5289 - acc: 1.0000 - val_loss: 37.1313 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 14.1485 - acc: 1.0000\n",
      "Epoch 00019: val_loss did not improve from 3.31258\n",
      "85783/85783 [==============================] - 16s 189us/sample - loss: 14.1164 - acc: 1.0000 - val_loss: 4.1228 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 3.1076 - acc: 1.0000\n",
      "Epoch 00020: val_loss improved from 3.31258 to 2.97656, saving model to Weights-020--2.97656.hdf5\n",
      "85783/85783 [==============================] - 16s 189us/sample - loss: 3.1071 - acc: 1.0000 - val_loss: 2.9766 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.8635 - acc: 1.0000\n",
      "Epoch 00021: val_loss improved from 2.97656 to 2.75570, saving model to Weights-021--2.75570.hdf5\n",
      "85783/85783 [==============================] - 16s 189us/sample - loss: 2.8631 - acc: 1.0000 - val_loss: 2.7557 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.6979 - acc: 1.0000\n",
      "Epoch 00022: val_loss improved from 2.75570 to 2.58341, saving model to Weights-022--2.58341.hdf5\n",
      "85783/85783 [==============================] - 17s 193us/sample - loss: 2.6976 - acc: 1.0000 - val_loss: 2.5834 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.6174 - acc: 1.0000\n",
      "Epoch 00023: val_loss improved from 2.58341 to 2.50702, saving model to Weights-023--2.50702.hdf5\n",
      "85783/85783 [==============================] - 17s 194us/sample - loss: 2.6169 - acc: 1.0000 - val_loss: 2.5070 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.4968 - acc: 1.0000\n",
      "Epoch 00024: val_loss improved from 2.50702 to 2.43473, saving model to Weights-024--2.43473.hdf5\n",
      "85783/85783 [==============================] - 17s 200us/sample - loss: 2.4967 - acc: 1.0000 - val_loss: 2.4347 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.3707 - acc: 1.0000\n",
      "Epoch 00025: val_loss improved from 2.43473 to 2.26727, saving model to Weights-025--2.26727.hdf5\n",
      "85783/85783 [==============================] - 17s 199us/sample - loss: 2.3702 - acc: 1.0000 - val_loss: 2.2673 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.3798 - acc: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 2.26727\n",
      "85783/85783 [==============================] - 15s 177us/sample - loss: 2.3799 - acc: 1.0000 - val_loss: 2.3590 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.2435 - acc: 1.0000\n",
      "Epoch 00027: val_loss improved from 2.26727 to 2.06749, saving model to Weights-027--2.06749.hdf5\n",
      "85783/85783 [==============================] - 15s 178us/sample - loss: 2.2435 - acc: 1.0000 - val_loss: 2.0675 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "85504/85783 [============================>.] - ETA: 0s - loss: 2.3359 - acc: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 2.06749\n",
      "85783/85783 [==============================] - 16s 181us/sample - loss: 2.3371 - acc: 1.0000 - val_loss: 2.8825 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "23040/85783 [=======>......................] - ETA: 10s - loss: 5.5429 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-69cbbcfa0bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m model.fit(x_train,y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2,\n\u001b[1;32m---> 12\u001b[1;33m           callbacks=[checkpoint, tensorboard,loss_history,lrate])\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode ='auto')\n",
    "\n",
    "model.compile(loss='mean_absolute_percentage_error',\n",
    "                  optimizer=optimiser,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2,\n",
    "          callbacks=[checkpoint, tensorboard,loss_history,lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'Weights-027--2.06749.hdf5' # choose the best checkpoint \n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss='mean_absolute_percentage_error', optimizer=optimiser, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict(x_test)\n",
    "\n",
    "# pred_b = in_city(pred)\n",
    "# y_test_b = in_city(y_test)\n",
    "\n",
    "# accuracy_f_score(pred_b,y_test_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
