{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook for taking the DNN trained in classifier_pretrained.ipynb and fine-tuning\n",
    "layers on final location data only.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_f_score(y_pred,y_true):\n",
    "    print(f\"Accuracy score: {round(accuracy_score(y_true, y_pred) * 100,2)}%\")\n",
    "    print('\\033[92m' + f\"F1 score: {f1_score(y_true, y_pred)}\" + '\\033[0m')\n",
    "    \n",
    "\n",
    "def in_city(x_pred,y_pred):\n",
    "    targets = []\n",
    "    \n",
    "    if (3750901.5068 <= x_pred <= 3770901.5069) and (-19268905.6133 <= y_pred <= -19208905.6133):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(1)\n",
    "    y = 1/(1+e**(-x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def journey_time(x,y):\n",
    "    \"\"\"\n",
    "    Compute journey time in seconds.\n",
    "    \"\"\"\n",
    "    x = pd.to_datetime(x)\n",
    "    y = pd.to_datetime(y)\n",
    "    return (y-x).total_seconds()\n",
    "\n",
    "def to_binary(x):\n",
    "    result = []\n",
    "    for n in x:\n",
    "        result.append(np.argmax(n))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_entry</th>\n",
       "      <th>y_entry</th>\n",
       "      <th>dist</th>\n",
       "      <th>net_tr</th>\n",
       "      <th>prev_tr</th>\n",
       "      <th>x_home</th>\n",
       "      <th>y_home</th>\n",
       "      <th>nj</th>\n",
       "      <th>j_time</th>\n",
       "      <th>dpc</th>\n",
       "      <th>final_loc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajectory_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>traj_0000a8602cf2def930488dee7cdad104_1_5</th>\n",
       "      <td>3.744945e+06</td>\n",
       "      <td>-1.928183e+07</td>\n",
       "      <td>45797.982227</td>\n",
       "      <td>99463.898797</td>\n",
       "      <td>3544.948847</td>\n",
       "      <td>3.751014e+06</td>\n",
       "      <td>-1.909398e+07</td>\n",
       "      <td>6.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>0.482047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0000cf177130469eeac79f67b6bcf3df_9_3</th>\n",
       "      <td>3.749088e+06</td>\n",
       "      <td>-1.926605e+07</td>\n",
       "      <td>29603.985176</td>\n",
       "      <td>-1056.813994</td>\n",
       "      <td>270.043451</td>\n",
       "      <td>3.749450e+06</td>\n",
       "      <td>-1.926506e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>0.497740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0001f97b99a80f18f62e2d44e54ef33d_3_1</th>\n",
       "      <td>3.758738e+06</td>\n",
       "      <td>-1.937594e+07</td>\n",
       "      <td>137051.659155</td>\n",
       "      <td>-1867.319643</td>\n",
       "      <td>-1867.319643</td>\n",
       "      <td>3.771461e+06</td>\n",
       "      <td>-1.910413e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>0.503453</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0002124248b0ca510dea42824723ccac_31_10</th>\n",
       "      <td>3.767866e+06</td>\n",
       "      <td>-1.917797e+07</td>\n",
       "      <td>61336.955341</td>\n",
       "      <td>5460.552001</td>\n",
       "      <td>-59655.060438</td>\n",
       "      <td>3.765544e+06</td>\n",
       "      <td>-1.917227e+07</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_000219c2a6380c307e8bffd85b5e404b_23_16</th>\n",
       "      <td>3.747641e+06</td>\n",
       "      <td>-1.922695e+07</td>\n",
       "      <td>17851.785279</td>\n",
       "      <td>-7113.420678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.760336e+06</td>\n",
       "      <td>-1.922818e+07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  x_entry       y_entry  \\\n",
       "trajectory_id                                                             \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    3.744945e+06 -1.928183e+07   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    3.749088e+06 -1.926605e+07   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    3.758738e+06 -1.937594e+07   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  3.767866e+06 -1.917797e+07   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  3.747641e+06 -1.922695e+07   \n",
       "\n",
       "                                                      dist        net_tr  \\\n",
       "trajectory_id                                                              \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     45797.982227  99463.898797   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3     29603.985176  -1056.813994   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    137051.659155  -1867.319643   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10   61336.955341   5460.552001   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16   17851.785279  -7113.420678   \n",
       "\n",
       "                                                  prev_tr        x_home  \\\n",
       "trajectory_id                                                             \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5     3544.948847  3.751014e+06   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3      270.043451  3.749450e+06   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    -1867.319643  3.771461e+06   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10 -59655.060438  3.765544e+06   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16      0.000000  3.760336e+06   \n",
       "\n",
       "                                                   y_home   nj  j_time  \\\n",
       "trajectory_id                                                            \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5   -1.909398e+07  6.0   962.0   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3   -1.926506e+07  4.0  1756.0   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1   -1.910413e+07  2.0  2716.0   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10 -1.917227e+07  9.0     0.0   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16 -1.922818e+07  8.0     0.0   \n",
       "\n",
       "                                                  dpc  final_loc  \n",
       "trajectory_id                                                     \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    0.482047          0  \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    0.497740          0  \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    0.503453          0  \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  1.000000          0  \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  0.500000          0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load data pertaining to final locations only.\n",
    "\"\"\"\n",
    "df = pd.read_csv('data_train/final_locations.csv')\n",
    "\n",
    "df[\"dpc\"] = list(map(sigmoid,df[\"dist_pct_ch\"])) #addresses skewness\n",
    "df.set_index(\"trajectory_id\", inplace=True)\n",
    "df[\"final_loc\"] = list(map(in_city, df[\"x_exit\"], df[\"y_exit\"]))\n",
    "\n",
    "y = df[\"final_loc\"].values\n",
    "df.drop([\"vmax\",\"vmin\",\"vmean\",\"time_entry\",\"time_exit\",\"hash\",\"dist_pct_ch\",\n",
    "         \"x_exit\",\"y_exit\"], axis=1, inplace=True)\n",
    "X = df.values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_entry</th>\n",
       "      <th>y_entry</th>\n",
       "      <th>dist</th>\n",
       "      <th>net_tr</th>\n",
       "      <th>prev_tr</th>\n",
       "      <th>x_home</th>\n",
       "      <th>y_home</th>\n",
       "      <th>nj</th>\n",
       "      <th>j_time</th>\n",
       "      <th>dpc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trajectory_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>traj_0000a8602cf2def930488dee7cdad104_1_5</th>\n",
       "      <td>6.573445</td>\n",
       "      <td>7.285148</td>\n",
       "      <td>4.660846</td>\n",
       "      <td>4.997665</td>\n",
       "      <td>3.549610</td>\n",
       "      <td>6.574149</td>\n",
       "      <td>7.280896</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>2.983175</td>\n",
       "      <td>-3.169107e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0000cf177130469eeac79f67b6bcf3df_9_3</th>\n",
       "      <td>6.573926</td>\n",
       "      <td>7.284793</td>\n",
       "      <td>4.471350</td>\n",
       "      <td>3.023999</td>\n",
       "      <td>2.431434</td>\n",
       "      <td>6.573968</td>\n",
       "      <td>7.284770</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>3.244525</td>\n",
       "      <td>-3.029973e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0001f97b99a80f18f62e2d44e54ef33d_3_1</th>\n",
       "      <td>6.575042</td>\n",
       "      <td>7.287263</td>\n",
       "      <td>5.136884</td>\n",
       "      <td>3.271219</td>\n",
       "      <td>3.271219</td>\n",
       "      <td>6.576510</td>\n",
       "      <td>7.281127</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>3.433930</td>\n",
       "      <td>-2.980409e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_0002124248b0ca510dea42824723ccac_31_10</th>\n",
       "      <td>6.576095</td>\n",
       "      <td>7.282803</td>\n",
       "      <td>4.787722</td>\n",
       "      <td>3.737237</td>\n",
       "      <td>4.775647</td>\n",
       "      <td>6.575828</td>\n",
       "      <td>7.282674</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.928655e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traj_000219c2a6380c307e8bffd85b5e404b_23_16</th>\n",
       "      <td>6.573758</td>\n",
       "      <td>7.283910</td>\n",
       "      <td>4.251682</td>\n",
       "      <td>3.852078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.575227</td>\n",
       "      <td>7.283938</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.010300e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              x_entry   y_entry      dist  \\\n",
       "trajectory_id                                                               \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    6.573445  7.285148  4.660846   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    6.573926  7.284793  4.471350   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    6.575042  7.287263  5.136884   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  6.576095  7.282803  4.787722   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  6.573758  7.283910  4.251682   \n",
       "\n",
       "                                               net_tr   prev_tr    x_home  \\\n",
       "trajectory_id                                                               \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    4.997665  3.549610  6.574149   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    3.023999  2.431434  6.573968   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    3.271219  3.271219  6.576510   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  3.737237  4.775647  6.575828   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  3.852078  0.000000  6.575227   \n",
       "\n",
       "                                               y_home        nj    j_time  \\\n",
       "trajectory_id                                                               \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5    7.280896  0.778151  2.983175   \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3    7.284770  0.602060  3.244525   \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1    7.281127  0.301030  3.433930   \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10  7.282674  0.954243  0.000000   \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16  7.283938  0.903090  0.000000   \n",
       "\n",
       "                                                      dpc  \n",
       "trajectory_id                                              \n",
       "traj_0000a8602cf2def930488dee7cdad104_1_5   -3.169107e-01  \n",
       "traj_0000cf177130469eeac79f67b6bcf3df_9_3   -3.029973e-01  \n",
       "traj_0001f97b99a80f18f62e2d44e54ef33d_3_1   -2.980409e-01  \n",
       "traj_0002124248b0ca510dea42824723ccac_31_10 -1.928655e-16  \n",
       "traj_000219c2a6380c307e8bffd85b5e404b_23_16 -3.010300e-01  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.apply(abs,axis=1)\n",
    "df = df.apply(np.log10,axis=1)\n",
    "df[df == -np.inf] = 0.0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X = df.values\n",
    "#X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0005\n",
    "    drop = 0.25\n",
    "    epochs_drop = 5\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=.0, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "NN_model = load_model('pretrained_model_wed.h5')\n",
    "#NN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.Dense object at 0x0000014F69A1BCC0> False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x0000014F69A545F8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000014F69A54518> False\n",
      "<keras.layers.core.Dropout object at 0x0000014F69A545C0> False\n",
      "<keras.layers.core.Dense object at 0x0000014F69A5BF28> False\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x0000014F69A54DA0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000014F71D9DB38> False\n",
      "<keras.layers.core.Dropout object at 0x0000014F71E16D30> True\n",
      "<keras.layers.core.Dense object at 0x0000014F71ECBBE0> True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x0000014F71F22FD0> True\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000014F71DE5CF8> True\n",
      "<keras.layers.core.Dropout object at 0x0000014F71F4B160> True\n",
      "<keras.layers.core.Dense object at 0x0000014F7202BE10> True\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x0000014F71F5E438> True\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000014F76E02B70> True\n",
      "<keras.layers.core.Dense object at 0x0000014F76E990F0> True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Freeze layers for fine-tuning -- \n",
    "training on all but the input layer(s) better than training on all layers or [-5].\n",
    "\"\"\"\n",
    "for layer in NN_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in NN_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107229 samples, validate on 26808 samples\n",
      "Epoch 1/100\n",
      "107229/107229 [==============================] - 48s 449us/step - loss: 0.0902 - acc: 0.9375 - val_loss: 0.0897 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08967, saving model to Weights-001--0.08967.hdf5\n",
      "Epoch 2/100\n",
      "107229/107229 [==============================] - 49s 460us/step - loss: 0.0894 - acc: 0.9379 - val_loss: 0.0889 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08967 to 0.08891, saving model to Weights-002--0.08891.hdf5\n",
      "Epoch 3/100\n",
      "107229/107229 [==============================] - 50s 470us/step - loss: 0.0893 - acc: 0.9379 - val_loss: 0.0894 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "107229/107229 [==============================] - 47s 441us/step - loss: 0.0893 - acc: 0.9383 - val_loss: 0.0892 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "107229/107229 [==============================] - 49s 458us/step - loss: 0.0886 - acc: 0.9387 - val_loss: 0.0886 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08891 to 0.08859, saving model to Weights-005--0.08859.hdf5\n",
      "Epoch 6/100\n",
      "107229/107229 [==============================] - 48s 450us/step - loss: 0.0884 - acc: 0.9386 - val_loss: 0.0883 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08859 to 0.08832, saving model to Weights-006--0.08832.hdf5\n",
      "Epoch 7/100\n",
      "107229/107229 [==============================] - 48s 452us/step - loss: 0.0885 - acc: 0.9384 - val_loss: 0.0882 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08832 to 0.08823, saving model to Weights-007--0.08823.hdf5\n",
      "Epoch 8/100\n",
      "107229/107229 [==============================] - 49s 461us/step - loss: 0.0883 - acc: 0.9387 - val_loss: 0.0883 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "107229/107229 [==============================] - 47s 440us/step - loss: 0.0881 - acc: 0.9388 - val_loss: 0.0884 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "107229/107229 [==============================] - 49s 458us/step - loss: 0.0881 - acc: 0.9387 - val_loss: 0.0882 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08823 to 0.08816, saving model to Weights-010--0.08816.hdf5\n",
      "Epoch 11/100\n",
      "107229/107229 [==============================] - 48s 452us/step - loss: 0.0878 - acc: 0.9387 - val_loss: 0.0881 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08816 to 0.08809, saving model to Weights-011--0.08809.hdf5\n",
      "Epoch 12/100\n",
      "107229/107229 [==============================] - 49s 453us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0881 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/100\n",
      "107229/107229 [==============================] - 49s 458us/step - loss: 0.0881 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08809 to 0.08803, saving model to Weights-013--0.08803.hdf5\n",
      "Epoch 14/100\n",
      "107229/107229 [==============================] - 48s 446us/step - loss: 0.0880 - acc: 0.9388 - val_loss: 0.0881 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "107229/107229 [==============================] - 49s 460us/step - loss: 0.0879 - acc: 0.9388 - val_loss: 0.0881 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "107229/107229 [==============================] - 48s 450us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0881 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "107229/107229 [==============================] - 50s 464us/step - loss: 0.0880 - acc: 0.9390 - val_loss: 0.0881 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "107229/107229 [==============================] - 50s 464us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "107229/107229 [==============================] - 48s 445us/step - loss: 0.0878 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08803 to 0.08803, saving model to Weights-019--0.08803.hdf5\n",
      "Epoch 20/100\n",
      "107229/107229 [==============================] - 51s 471us/step - loss: 0.0877 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08803 to 0.08802, saving model to Weights-020--0.08802.hdf5\n",
      "Epoch 21/100\n",
      "107229/107229 [==============================] - 51s 478us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08802 to 0.08802, saving model to Weights-021--0.08802.hdf5\n",
      "Epoch 22/100\n",
      "107229/107229 [==============================] - 51s 472us/step - loss: 0.0878 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/100\n",
      "107229/107229 [==============================] - 49s 460us/step - loss: 0.0879 - acc: 0.9391 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/100\n",
      "107229/107229 [==============================] - 48s 445us/step - loss: 0.0878 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/100\n",
      "107229/107229 [==============================] - 49s 461us/step - loss: 0.0879 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/100\n",
      "107229/107229 [==============================] - 48s 445us/step - loss: 0.0877 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/100\n",
      "107229/107229 [==============================] - 50s 464us/step - loss: 0.0876 - acc: 0.9393 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/100\n",
      "107229/107229 [==============================] - 48s 450us/step - loss: 0.0878 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/100\n",
      "107229/107229 [==============================] - 48s 447us/step - loss: 0.0878 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/100\n",
      "107229/107229 [==============================] - 51s 473us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/100\n",
      "107229/107229 [==============================] - 52s 483us/step - loss: 0.0878 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/100\n",
      "107229/107229 [==============================] - 54s 505us/step - loss: 0.0876 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/100\n",
      "107229/107229 [==============================] - 53s 492us/step - loss: 0.0878 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/100\n",
      "107229/107229 [==============================] - 55s 509us/step - loss: 0.0877 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/100\n",
      "107229/107229 [==============================] - 54s 507us/step - loss: 0.0875 - acc: 0.9392 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/100\n",
      "107229/107229 [==============================] - 58s 545us/step - loss: 0.0877 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/100\n",
      "107229/107229 [==============================] - 56s 524us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/100\n",
      "107229/107229 [==============================] - 54s 506us/step - loss: 0.0878 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/100\n",
      "107229/107229 [==============================] - 53s 496us/step - loss: 0.0877 - acc: 0.9394 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/100\n",
      "107229/107229 [==============================] - 52s 484us/step - loss: 0.0878 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/100\n",
      "107229/107229 [==============================] - 54s 507us/step - loss: 0.0880 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/100\n",
      "107229/107229 [==============================] - 56s 521us/step - loss: 0.0877 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/100\n",
      "107229/107229 [==============================] - 55s 512us/step - loss: 0.0879 - acc: 0.9393 - val_loss: 0.0880 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/100\n",
      "107229/107229 [==============================] - 55s 510us/step - loss: 0.0876 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/100\n",
      "107229/107229 [==============================] - 59s 550us/step - loss: 0.0876 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/100\n",
      "107229/107229 [==============================] - 53s 498us/step - loss: 0.0877 - acc: 0.9392 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/100\n",
      "107229/107229 [==============================] - 51s 475us/step - loss: 0.0877 - acc: 0.9392 - val_loss: 0.0880 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/100\n",
      "107229/107229 [==============================] - 55s 515us/step - loss: 0.0880 - acc: 0.9386 - val_loss: 0.0880 - val_acc: 0.9377\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/100\n",
      "107229/107229 [==============================] - 53s 491us/step - loss: 0.0878 - acc: 0.9389 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/100\n",
      "107229/107229 [==============================] - 52s 489us/step - loss: 0.0877 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/100\n",
      "107229/107229 [==============================] - 54s 505us/step - loss: 0.0879 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/100\n",
      "107229/107229 [==============================] - 56s 525us/step - loss: 0.0877 - acc: 0.9385 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/100\n",
      "107229/107229 [==============================] - 55s 515us/step - loss: 0.0878 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08802 to 0.08802, saving model to Weights-053--0.08802.hdf5\n",
      "Epoch 54/100\n",
      "107229/107229 [==============================] - 55s 516us/step - loss: 0.0879 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/100\n",
      "107229/107229 [==============================] - 73s 677us/step - loss: 0.0878 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/100\n",
      "107229/107229 [==============================] - 84s 785us/step - loss: 0.0878 - acc: 0.9387 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/100\n",
      "107229/107229 [==============================] - 69s 643us/step - loss: 0.0877 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/100\n",
      "107229/107229 [==============================] - 83s 773us/step - loss: 0.0877 - acc: 0.9393 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/100\n",
      "107229/107229 [==============================] - 87s 813us/step - loss: 0.0877 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/100\n",
      "107229/107229 [==============================] - 85s 795us/step - loss: 0.0877 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/100\n",
      "107229/107229 [==============================] - 105s 978us/step - loss: 0.0878 - acc: 0.9393 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/100\n",
      "107229/107229 [==============================] - 101s 946us/step - loss: 0.0877 - acc: 0.9392 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/100\n",
      "107229/107229 [==============================] - 65s 609us/step - loss: 0.0880 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/100\n",
      "107229/107229 [==============================] - 50s 466us/step - loss: 0.0876 - acc: 0.9390 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/100\n",
      "107229/107229 [==============================] - 49s 459us/step - loss: 0.0877 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/100\n",
      "107229/107229 [==============================] - 47s 440us/step - loss: 0.0880 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/100\n",
      "107229/107229 [==============================] - 53s 495us/step - loss: 0.0878 - acc: 0.9388 - val_loss: 0.0880 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/100\n",
      "  4864/107229 [>.............................] - ETA: 50s - loss: 0.0863 - acc: 0.9410"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7b34b067c516>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m NN_model.fit(x_train, y_train, epochs=100, batch_size=256, validation_split = 0.2,\n\u001b[1;32m---> 10\u001b[1;33m              callbacks=callbacks_list, class_weight=class_weight)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint,loss_history,lrate]\n",
    "class_weight = {0: .5,\n",
    "                1: .66}\n",
    "y_binary = to_categorical(y)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y_binary,test_size=0,random_state=410,shuffle=True)\n",
    "\n",
    "NN_model.fit(x_train, y_train, epochs=100, batch_size=256, validation_split = 0.2,\n",
    "             callbacks=callbacks_list, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'Weights-053--0.08802.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(weights_file) # load it\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make predictions if using only a sub-sample of the training data\"\"\"\n",
    "\n",
    "X_test = df_test.values\n",
    "# X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "pred = NN_model.predict(X_test)\n",
    "pred_b = to_binary(pred)\n",
    "ids = df_test.index\n",
    "results_df = pd.DataFrame()\n",
    "results_df[\"id\"] = ids\n",
    "results_df[\"target\"] = pred_b\n",
    "\n",
    "results_df.to_csv(\"../results/classifier_TL_b1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
